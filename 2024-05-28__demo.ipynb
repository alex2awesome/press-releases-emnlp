{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8bafcf-8e7c-425c-a624-a54cc46e6861",
   "metadata": {},
   "source": [
    "# In this demo, we walk through how to retrieve and use our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebacb1-3a56-4de9-a72b-42690fb441e9",
   "metadata": {},
   "source": [
    "As described in the paper, we discovered press release/article pairs through two primary directions: __forwards__ (i.e. crawling news articles and discovering hyperlinks to press releases) and __backwards__ (i.e. crawling press releases and querying a backlink service to discover news articles.) \n",
    "\n",
    "We show how to access the data collected from each direction now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37fe42-1a76-4809-9ef9-637496bd071b",
   "metadata": {},
   "source": [
    "## Forwards Direction: Press Releases $\\leftarrow$ News Articles (Hyperlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcead243-0f38-4daa-b93a-ca96dcc5c58d",
   "metadata": {},
   "source": [
    "The data for this direction is primarily stored in a SQLite3 database: `data/article_to_press_release_data.db.tar.gz`\n",
    "\n",
    "It can be accessed from this [Google Drive link](https://drive.google.com/drive/folders/1CmJkwpbV84pYaEMtNWhWkzG-B4_3hXR8?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200c4cb-419a-4cfe-a9f0-e0e19ac79a94",
   "metadata": {},
   "source": [
    "## Backwards Direction: Press Releases $\\rightarrow$ News Articles  (Backlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20e049-d5d4-4adc-af2f-ba74f6892526",
   "metadata": {},
   "source": [
    "The data for this direction is primarily stored in three files:\n",
    "\n",
    "They can be accessed from this [Google Drive Link](https://drive.google.com/drive/folders/11IpwmFKuFn7LryUHW1df1fcJ2RmFUub1?usp=drive_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b995f-81d0-4537-9ca5-d54258c4f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "data_dir = '../data'\n",
    "source_df = pd.read_json(f'{data_dir}/full-source-scored-data.jsonl.gz', lines=True, compression='gzip', nrows=5000)\n",
    "article_d = load_from_disk(f'{data_dir}/all-coref-resolved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6b6ba9-97ec-44a9-ac9f-fd7a247be1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article_url', 'target_timestamp_key', 'target_timestamp', 'sort_criteria', 'wayback_url', 'wayback_timestamp', 'method', 'links', 'article_text', 'word_lists', 'sent_lists', 'best_class', 'coref_resolved_sents'],\n",
       "    num_rows: 496380\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972d02d-6e86-48a6-a3d9-9a16f788e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_urls = set(source_df['article_url'])\n",
    "filtered_article_d = article_d.filter(lambda x: x['article_url'] in a_urls, num_proc=10)\n",
    "filtered_article_df = filtered_article_d.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60e22d78-0699-4e0a-ab80-e2ff1562154e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disallowed_quote_types = set(['Other', 'Background/Narrative', 'No Quote'])\n",
    "\n",
    "sentences_with_quotes = (\n",
    "    filtered_article_d\n",
    "         .to_pandas()\n",
    "         .merge(source_df, on='article_url')\n",
    "         [['article_url', 'attributions', 'quote_type', 'sent_lists',]]\n",
    "         .explode(['attributions', 'quote_type', 'sent_lists'])\n",
    ")\n",
    "\n",
    "sentences_with_quotes = (sentences_with_quotes\n",
    "     .assign(attributions=lambda df: \n",
    "             df.apply(lambda x: x['attributions'] if x['quote_type'] not in disallowed_quote_types else np.nan, axis=1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ccb6c-b88f-4e33-b7b0-b6e747323bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_article = (\n",
    "    sentences_with_quotes\n",
    "         .loc[lambda df: df['article_url'] == df['article_url'].unique()[1]]\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "doc_str = one_article[['sent_lists', 'attributions']].to_csv(sep='\\t', index=False)\n",
    "json_str = one_article[['sent_lists', 'attributions']].to_json(lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6677c864-4dda-4789-b0d7-cffaba8ebdc1",
   "metadata": {},
   "source": [
    "# Finding Contrasting Press Releases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c8bb0-0f30-4ff0-9ff0-d89589d12d8d",
   "metadata": {},
   "source": [
    "In our paper, we described a method to find news articles that provided \"critical coverage\" of press releases. This method used aggregated sentence-level NLI scores on the document level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25567c0-ed29-412a-aeed-aede75a3c2e3",
   "metadata": {},
   "source": [
    "![alt text](images/doc-level-nli.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f57c2e8-013c-4e8e-9c0b-cb011a87f5d1",
   "metadata": {},
   "source": [
    "for more information, see `src/assess_factual_consistency.py`. However, we do recommend taking a different, more modern approach, possibly one that utilizes LLMs to distill a BERT-based classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42420e88-f60a-48cd-9e45-fbe32bd6a797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f13e02f-ed81-4b46-ac52-cf5189513589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe0757f4-24f7-404e-8078-d4c5c0a31374",
   "metadata": {},
   "source": [
    "# Extracting Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7462f6-678e-4cab-89f6-dde039a4e314",
   "metadata": {},
   "source": [
    "I ran llama3 locally using a cool program called Ollama: https://ollama.com/. You can follow the link to install the program.\n",
    "\n",
    "This can be useful for trying things out before we work out compute access.\n",
    "\n",
    "Run this in your terminal:\n",
    "\n",
    "`ollama run llama3`\n",
    "\n",
    "Even llama3 8b is pretty powerful. Llama 70b is better, but that may not run on your local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5141b2f-3912-418c-800e-f7ed290a450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa3d76-9fc8-4483-b542-c8844b2a33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyperclip.copy(doc_str)\n",
    "pyperclip.copy(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53c3a43e-c0a0-4e25-a048-2459cf4dbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    'http://localhost:11434/api/generate', \n",
    "    json = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\":f\"\"\"\n",
    "            Here is a news article, with each sentence annotated according to the source of it's information:\n",
    "            ```\n",
    "            {json_str}\n",
    "            ```\n",
    "\n",
    "            Please summarize each of our source annotations. Tell me in one paragraph per source: (1) who the source is (2) what informational content they provide to the article. \n",
    "            Only rely on the annotations I have provided, don't identify additional sources.\n",
    "        \"\"\",\n",
    "        \"stream\": False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "badef086-20f0-4aa9-9769-7c852ff9b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Socrata Foundation**: The Socrata Foundation is a 501(c)(3) organization that provides information about its philanthropic philosophy and mission. They describe their support for unique organizations that lack resources or financial means to fulfill their data-driven missions. They also mention the importance of open data in removing barriers to social justice and economic progress.\n",
      "\n",
      "**Robert Runge**: Robert Runge, a member of Socrata's Board of Directors, provides additional context about the Socrata Foundation's purpose and how it bridges the gap between publicly funded open data projects and underfunded or unfunded opportunities.\n",
      "\n",
      "**Mike Duggan**: Detroit Mayor Mike Duggan shares his perspective on why he turned to data-driven government in Detroit. He highlights the importance of transparency, accountability, and fact-based decision-making enabled by open data. He also explains how the Socrata Foundation helped Detroit gain access to the necessary technology and infrastructure.\n",
      "\n",
      "**Heat Seek NYC Founder William Jeffries**: Heat Seek NYC's founder, William Jeffries, provides insight into the organization's mission and goals. He emphasizes the importance of gathering data to protect tenants from unsafe living conditions and shares his vision for using this data to hold accountable those responsible for heating violations in New York City.\n",
      "\n",
      "Note: There are no other sources annotated besides these four (Socrata Foundation, Robert Runge, Mike Duggan, and William Jeffries).\n"
     ]
    }
   ],
   "source": [
    "print(r.json()['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4663f71-4708-435f-a1bf-3cdff4b994b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8141d3c-db08-4205-9e22-48b08db25e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918831a4-f609-4f7d-a36e-8abab527cfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e1ddc-20cb-4b43-b573-231f813b9c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f20973-c1a7-422c-924d-4855b9fe8d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2abe74-47ae-49ca-a08c-c792ba120863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab063171-471b-49a6-a61b-ec5d0e4ba98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adba74-fe9f-4435-9047-8798fc97593c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
